<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">

<!--Converted with LaTeX2HTML 2008 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Development of an OTA (Over the Air) Mobile Learning Telepresence Platform</TITLE>
<META NAME="description" CONTENT="Development of an OTA (Over the Air) Mobile Learning Telepresence Platform">
<META NAME="keywords" CONTENT="mastersthesis">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=utf-8">
<META NAME="Generator" CONTENT="LaTeX2HTML v2008">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="mastersthesis.css">

</HEAD>

<BODY >

	<DIV ALIGN="CENTER">

	
<BR>
	<SMALL>BY
</SMALL><BR>
	
	
<BR>
	HBS<SMALL>C </SMALL>C<SMALL>OMPUTER </SMALL>S<SMALL>CIENCE, </SMALL>L<SMALL>AKEHEAD </SMALL>U<SMALL>NIVERSITY, 2013</SMALL><BR>
<BR>
</DIV>
<P>
<DIV ALIGN="CENTER">A T<SMALL>HESIS </SMALL>S<SMALL>UBMITTED IN </SMALL>P<SMALL>ARTIAL </SMALL>F<SMALL>ULFILLMENT OF THE </SMALL>R<SMALL>EQUIREMENTS FOR THE </SMALL>D<SMALL>EGREE OF</SMALL><BR>
<BR>
	MSC COMPUTER SCIENCE
<BR>
	<SMALL>IN THE DEPARTMENT OF </SMALL>C<SMALL>OMPUTER </SMALL>S<SMALL>CIENCE</SMALL><BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
</DIV>
<P>
<DIV ALIGN="CENTER">C<SMALL>OPYRIGHT </SMALL>K<SMALL>YLE </SMALL>G<SMALL>ALVIN, 2013
</SMALL><BR>
	L<SMALL>AKEHEAD </SMALL>U<SMALL>NIVERSITY</SMALL><BR>
<BR>
	A<SMALL>LL RIGHTS RESERVED. </SMALL>T<SMALL>HIS THESIS MAY NOT BE REPRODUCED IN WHOLE OR IN PART, PY PHOTOCOPY OR OTHER MEANS, WITHOUT THE PERMISSION OF THE AUTHOR.</SMALL>
	
</DIV>

	<DIV ALIGN="CENTER">
<BIG CLASS="XXLARGE">S<SMALL>UPERVISORY </SMALL>C<SMALL>OMMITTEE</SMALL><BR>
<BR>
<BR>
<BR>
<BR>
<BR></BIG>
		
<BR>
		<SMALL>BY
</SMALL><BR>
		
<BR>
		HBS<SMALL>C </SMALL>C<SMALL>OMPUTER </SMALL>S<SMALL>CIENCE, </SMALL>L<SMALL>AKEHEAD </SMALL>U<SMALL>NIVERSITY, 2013</SMALL><BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
<BR>
	
</DIV>
	<BIG CLASS="XLARGE">S<SMALL>UPERVISORY </SMALL>C<SMALL>OMMITTEE</SMALL><BR>
<BR></BIG>
	J<SMALL>INAN </SMALL>F<SMALL>IAIDHI AND </SMALL>S<SMALL>ABAH </SMALL>M<SMALL>OHAMMED
</SMALL><BR>
	<SPAN  CLASS="textit">S<SMALL>UPERVISOR</SMALL></SPAN>
<BR>
	<SPAN  CLASS="textit">C<SMALL>O-</SMALL>S<SMALL>UPERVISOR OR </SMALL>D<SMALL>EPARTMENTAL </SMALL>M<SMALL>EMBER</SMALL></SPAN>
<BR>
	<SPAN  CLASS="textit">D<SMALL>EPARTMENTAL </SMALL>M<SMALL>EMBER</SMALL></SPAN>
<BR>
	<SPAN  CLASS="textit">O<SMALL>UTSIDE </SMALL>M<SMALL>EMBER</SMALL></SPAN>
<BR>

<P>
<BR>

<H2><A NAME="SECTION00100000000000000000">
Contents</A>
</H2>
<!--Table of Contents-->

<UL CLASS="TofC">
<LI><A NAME="tex2html39"
  HREF="mastersthesis.html#SECTION00300000000000000000">Lightweight Telepresence Technologies</A>
<UL>
<LI><A NAME="tex2html40"
  HREF="mastersthesis.html#SECTION00310000000000000000">Emerging Mobile Technologies</A>
<UL>
<LI><A NAME="tex2html41"
  HREF="mastersthesis.html#SECTION00311000000000000000">Microcontrollers &amp; customized System on a Chip (SoaC) components</A>
<LI><A NAME="tex2html42"
  HREF="mastersthesis.html#SECTION00312000000000000000">Mobile phones &amp; Cellular devices</A>
</UL>
<LI><A NAME="tex2html43"
  HREF="mastersthesis.html#SECTION00320000000000000000">Telepresence &amp; Real-Time communications</A>
<UL>
<LI><A NAME="tex2html44"
  HREF="mastersthesis.html#SECTION00321000000000000000">Audio/Video compression</A>
<LI><A NAME="tex2html45"
  HREF="mastersthesis.html#SECTION00322000000000000000">Cellular network bandwidth flow &amp; optimization</A>
<LI><A NAME="tex2html46"
  HREF="mastersthesis.html#SECTION00323000000000000000">Privacy</A>
<LI><A NAME="tex2html47"
  HREF="mastersthesis.html#SECTION00324000000000000000">Cloud Based Assisted Technologies</A>
</UL>
<LI><A NAME="tex2html48"
  HREF="mastersthesis.html#SECTION00330000000000000000">Digital identification and modeling</A>
<UL>
<LI><A NAME="tex2html49"
  HREF="mastersthesis.html#SECTION00331000000000000000">Bar codes &amp; QR codes</A>
<LI><A NAME="tex2html50"
  HREF="mastersthesis.html#SECTION00332000000000000000">RFID; NFC</A>
<LI><A NAME="tex2html51"
  HREF="mastersthesis.html#SECTION00333000000000000000">real-time digital modeling</A>
<LI><A NAME="tex2html52"
  HREF="mastersthesis.html#SECTION00334000000000000000">Image recognition and classification</A>
</UL>
<LI><A NAME="tex2html53"
  HREF="mastersthesis.html#SECTION00340000000000000000">Identifying the Major Elements for a Lightweight Mobile Telepresence System</A>
<LI><A NAME="tex2html54"
  HREF="mastersthesis.html#SECTION00350000000000000000">Summary</A>
</UL><BR>
<LI><A NAME="tex2html55"
  HREF="mastersthesis.html#SECTION00400000000000000000">Design Document</A>
<UL>
<LI><A NAME="tex2html56"
  HREF="mastersthesis.html#SECTION00410000000000000000">Goals</A>
<LI><A NAME="tex2html57"
  HREF="mastersthesis.html#SECTION00420000000000000000">Design</A>
<LI><A NAME="tex2html58"
  HREF="mastersthesis.html#SECTION00430000000000000000">Networking</A>
<UL>
<LI><A NAME="tex2html59"
  HREF="mastersthesis.html#SECTION00431000000000000000">I/O Limitations</A>
<LI><A NAME="tex2html60"
  HREF="mastersthesis.html#SECTION00432000000000000000">Formal Grammar</A>
</UL>
<LI><A NAME="tex2html61"
  HREF="mastersthesis.html#SECTION00440000000000000000">Hardware</A>
<LI><A NAME="tex2html62"
  HREF="mastersthesis.html#SECTION00450000000000000000">Digital Signal Processing</A>
<UL>
<LI><A NAME="tex2html63"
  HREF="mastersthesis.html#SECTION00451000000000000000">Components</A>
</UL>
</UL><BR>
<LI><A NAME="tex2html64"
  HREF="mastersthesis.html#SECTION00500000000000000000">Bibliography</A>
</UL>
<!--End of Table of Contents-->
<P>

<H1><A NAME="SECTION00200000000000000000">
<DIV ALIGN="CENTER">

</DIV>Abstract</A>
</H1>
Telepresence has been used in many forms in academia university for more than a decade by now as entities that help to maintain the relationship with learners and provide them with collaborative experiences without the expense of physical travel. However, the emerging technology has shifted its focus from the large class-room telepresence equipment’s to be scaled- down to mobile, wireless-networked telepresence products. With this technology shift, we are required to provide the learner with the ubiquitous ability to explore core learning contents deployed over the internet as well as to enable learners to interact with many other remote physical learning environments (e.g., Web environments, instructors, colleagues) through the use of mobile devices. This project aims at exploring this research area and to come with a solution for implementing a new type of learning objects that can be used over the air for telecollaboration and telepresence suitable for mobile platforms. 

<P>

<H1><A NAME="SECTION00300000000000000000">
Lightweight Telepresence Technologies</A>
</H1>

<P>
Microprocessors have shaped the world over the last century. Reducing in size over time exponentially, we are now able to achieve things that would have been unimaginable in the past. We can squeeze more bits per volume, transport more information and crunch more data each second than ever before. With this explosion of portability and connectivity comes a renaissance of technological growth that is unfolding before our eyes.

<P>
Density of information and computation as well as the speed of communication are at the core of modern digital technology, yet focusing on these features displays a very hands-on white box approach. There is also much to be learned with respect to the interaction between digital components and their environments, which could be considered more of a black box "I/O" style description. The interface a device supplies for others to interact with is just as important as the computational and communicative abilities the device has intenally to process the environment around it.

<P>
By extrapolating on current computational growth trends, we can imagine many applications in which technology will soon improve our every day lives. By studying these applications both mundane and whimsical alike, we are likely to find many exciting ideas which are attainable much more immediately than they first appeared.

<P>
Arthur C. Clarke once wrote "Any sufficiently advanced technology is indistinguishable from magic". Indeed many amazing discoveries can find roots in sci-fi and futuristic predictions which push the boundaries of our collective knowlege and explore the potential and logical conclusions of current technological progress. The most recent ideas which are moving from science fiction to science fact are telepresence and augmented reality. 

<P>
To introduce these ideas, I will borrow from the definitions others have supplied:
<BLOCKQUOTE>
Telepresence systems provide a human operator with the feeling of actual presence in a remote environment, the target environment. The feeling of presence is achieved by visual and acoustic sensory information recorded from the target environment and presented to the user on an immersive display.
	
<DIV ALIGN="RIGHT">
[<A
 HREF="mastersthesis.html#6094998">AH11</A>]
	
</DIV>
</BLOCKQUOTE>

<P>
Likewise, an accurate and compelling description of augmented reality:

<P>
<BLOCKQUOTE>
AR is a variation of the more known concept of Virtual Reality Technology (VR), which is often defined as “the use of real-time digital computers and other special hardware and software to generate a simulation of an alternate world or environment, which is believable as real or true by the users”. VR technology creates an environment in which the user feels and seems to be moving inside a computer-created virtual world in the same way people move inside natural environment; while immersed in the virtual world, the user cannot perceive the real one which still surrounds him. On the contrary, AR allows the user to see the real world, augmenting it with superimposed virtual objects. In other words, while VR replaces reality, AR supplements it, creating an environment in which real and virtual objects harmonically coexist.
	
<DIV ALIGN="RIGHT">
[<A
 HREF="mastersthesis.html#5970856">HCRM11</A>]
	
</DIV>
</BLOCKQUOTE>

<P>

<H1><A NAME="SECTION00310000000000000000">
Emerging Mobile Technologies</A>
</H1>
As microchip density increases, so does the mobility of computational and processing devices. While PDA and handheld gaming devices have been around for decades, the advance of cellular networks which allow for on-the-go personal telecommunications and widely dispersed access to internet services has really driven the shape and design of the current generation of mobile devices.

<P>
Smart phones and telecommunications aren't the only technology in this arena, but they are certainly the largest and most influential. Other devices to consider when discussing telepresence devices are lightweight microprocessors and system on a chip designs. These devices can allow industry and hobbiests alike to create a wide array of telepresence hardware that is capable of interacting with the environment around it on another's behalf. In this case, we are now less bound by strict computational limits and are now merely bound by the sensors, motors, and analog/digital conversions available to read from (and interact with) the environment around us.

<P>
When we combine our new-found freedom to invent any sort of sensory device with our fully connected and always online 'internet of things', we can begin to explore and create all sorts of ideas that were inaccessable to the real-world and thus bound to the realm of fiction, futurism, and sci-fi.

<H2><A NAME="SECTION00311000000000000000">
Microcontrollers &amp; customized System on a Chip (SoaC) components</A>
</H2>

<P>
Microcontrollers have become the de-facto platform for lightweight, mobile, and miniaturized devices. Much of the power of microcontrollers has been achieved by clever specializations and optimizations of the CPU unit. By diverging from the Desktop model where raw power takes precedence over power consuption, there are now a variety of architectures and specialized components that are well-suited to mobility. In fact there is now a wide spectrum of hardware ranging from high performance to low power consumption. With server and desktop harware on one end of the spectrum, we have recently expanded the power efficient end with a range of miniaturized ARM general-purpose CPUs capable of of supporting a general-purpose operating system and related peripheral components in an extremely small enclosure. To continue down the spectrum we depart from a traditional operating system and move towards programmable integrated circuits (PIC) and pure-hardware components which perform more specialized tasks using even less space and power.

<P>
Because of this balance between portability and power, it is important to keep in mind the practical limitations on the applications a device can support. While a computationally demanding task such as image processing, we are unlikely to get satisfactory results with a PIC controller, yet the current generation of ARM controllers which are recently emerging are just beginning to practically handle these tasks.

<P>
An interesting idea to consider is delegation of the heavy processing to a more capable device. However, this strategy simply moves the problem from the computational I/O boundaries of the device towards the networking I/O boundaries. To achieve practical results, we can use a hybrid approach by allowing the device to pre-process the data (assuming such a pre-processing can reduce the size of the raw data, preferrably an order of magnitude or more) before sending the derived result over the network to a more capable device.

<P>
Moving from our internal capabilities towards our external interactivity, we are capable of hooking up a wide array of sensors, displays, and feedback devices. From motors to spectroscopy sensors to audio capture and processing (and many, many more) the possibilities are limited only by the fidelity/accuracy of the sensors around us and our own creativity.

<P>
Before we wander into fanciful and eclectic descriptions of advanced robots and artificial intelligence which could easily be the plot of the next sci-fi thriller, we should take note of the challenges and limitations of today's robotic and so-called 'smart' devices. For example, to apply the concept of tele-robotics (where the telepresence user is represented and mimiced as a robotic avatar on the opposite end) the following challenges have been identified and discussed:

<P>
<BLOCKQUOTE>
In this telepresence domain, as remote manipulators become more sophisticated and the tasks they undertake become more complex. Three main sensory obstacles to effective widespread use have been identified [15]:
	</BLOCKQUOTE>
<OL>
<LI>The detection of sensory signals
</LI>
<LI>The feedback of real-time sensory information to the operator
</LI>
<LI>The presentation of this information in a form that can be easily detected, processed by the brain as a reflex action and responded to, since an excessive need for thought would detract from performance of the primary task.
	
</LI>
</OL><BLOCKQUOTE>
	
<DIV ALIGN="RIGHT">
[<A
 HREF="mastersthesis.html#540147">CWKG96</A>]
	
</DIV>
</BLOCKQUOTE>

<P>
Aside from end-to-end synchronization and operator awareness, the question of usefulness and usability must also be addressed. Applicability has been fomrmally described as follows:

<P>
<P>
<BLOCKQUOTE>The main objective of tele-robotics has been to develop methodologies for the control of robots at remote sites by human users at local sites. Tele-robots are suitable in certain situations such as:
</BLOCKQUOTE>
<OL>
<LI>The robot must operate in environments that are hazardous to human health.
</LI>
<LI>The robot must operate at a scale that is much smaller or larger than the human size and scale.
</LI>
<LI>The robot must operate in a location where it would be too costly for the human to be present (in terms of budget, timing requirements, and human safety).
</LI>
</OL><BLOCKQUOTE>
	
<DIV ALIGN="RIGHT">
[<A
 HREF="mastersthesis.html#726589">AWZ98</A>]
	
</DIV>
</BLOCKQUOTE>

<P>
It begins to become clear that tele-robotics is not an idea that would lend itself to any situation. The reduction in fidelity we introduce by using a proxy or avatar robot is severe enough that we have only found successful use-cases in well-defined environments such as laboratories and factories where there are little or no unknown variables. Situations where there are people in close proximity to the robots are often too dangerous for the technology to be applied, and furthermore the decrease in the awareness a user would have with their remote surroundings makes the potential for interaction very limited. This barrier to adoption can be seen echoed in the following passage:

<P>
<BLOCKQUOTE>
Traditionally most robotic applications have involved the use of single static (non-mobile) manipulator platforms, with this technique being particularly suited to applications where the actual task is relatively well defined, the work volume is limited and safety considerations make even slightly “unexpected” motions totally unacceptable.
	
<DIV ALIGN="RIGHT">
[<A
 HREF="mastersthesis.html#540147">CWKG96</A>]
	
</DIV>
</BLOCKQUOTE>

<P>
<BLOCKQUOTE>

	...the BCI collects the EEG brain activity and decodes the user's intentions, which are transferred to the robot via the Internet. The robot autonomously executes the orders using the navigation system (implemented with a combination of dynamic online grid maping with scan matching, dynamic path planning, and obstacle avoidance) or the camera orientation system. Thus, the shared-control strategy is built by means  of the mental selection of robot navigation or active visual exploration task-related orders, which can be autonomously executed by the robot.
	
<DIV ALIGN="RIGHT">
[<A
 HREF="mastersthesis.html#6104414">EAM12</A>]
	
</DIV>
</BLOCKQUOTE>

<P>
The idea of presence fidelity can be considered a continuum. We can consider verbal descriptions and printed material on the low-fidelity end of the spectrum, while actual presence would be on the highest end. Our goal is to bring virtual telepresence farther along this continuum until it is as close to actual presence as possible [<A
 HREF="mastersthesis.html#726589">AWZ98</A>]

<P>

<H2><A NAME="SECTION00312000000000000000">
Mobile phones &amp; Cellular devices</A>
</H2> When discussing mobile telepresence, the recent emergence of smart phones is undeniably the most important event to occur to date. In a learning environment they are indespensable for personal organization, time managment, and real-time updates. It has been stated by others that they can now allow students to ''...be informed of all necessary notices, assignment deadlines and supervisor advices during their busy schedule''. In fact, ''Mobile education is defined ... as any service or facility that supplies a learner with general electronic information and educational content that aids in acquisition of knowledge regardless of location and time.'' [<A
 HREF="mastersthesis.html#4469080">Hos07</A>]

<P>
[<A
 HREF="mastersthesis.html#6007847">Bin11</A>]

<H1><A NAME="SECTION00320000000000000000">
Telepresence &amp; Real-Time communications</A>
</H1>

<H2><A NAME="SECTION00321000000000000000">
Audio/Video compression</A>
</H2>
[<A
 HREF="mastersthesis.html#4297087">SlZSSyX07</A>]
[<A
 HREF="mastersthesis.html#4801602">LLD09</A>]

<P>
Within the context of mobile devices we need to consider not only the computational complexity and bandwidth consumption of video streaming, but the power consumption of the encoder as well.
[<A
 HREF="mastersthesis.html#5054795">AKK09</A>]

<H2><A NAME="SECTION00322000000000000000">
Cellular network bandwidth flow &amp; optimization</A>
</H2>

<P>
Network traffic can be prioritized using QoS (Quality of Service) classification. By prioritizing traffic types into real-time and non-real-time categories we can decrease the average latency on timing-critical services. By extending these concepts and including cooperative game theory strategies (by exploring Nash Bargaining Systems). A game theory strategy can be broken down into three components: Players, Strategies, and Interactions (or game utilities). If we can determine a metric for success in the context of each player, than we can create strategies which each player can use to interact with the system in a way which optimizes the collective success of all the players. Applied to bandwidth optimization, each network device is a player which utilizes a strategy for sharing the limited resources of the network.

<P>
The trend among wireless networks is an increased number of cells over smaller and smaller areas serving users. By reducing this cell size, we introduce an increased number of hand offs when the user is mobile. When a user passes from the range of one operator to the next, this hand-off should not interrupt the user's communication. Because of this, our QoS strategies should include a percentage of bandwidth reserved for hand-off services. 

<P>
With many different use-cases and bandwidth categories our goal is to create a strategy which optimizes the usability of all devices on the network. By ordering our bandwidth categories by highest to lowest priority we can allocate each users traffic into their respective categories while also dynamically adjusting the maximum flow of each category to reflect (as well as prioritize) the immediate demand in real-time
[<A
 HREF="mastersthesis.html#5710522">Kim11</A>]
[<A
 HREF="mastersthesis.html#1300874">KV04</A>]
[<A
 HREF="mastersthesis.html#1376696">LYC04</A>]

<H2><A NAME="SECTION00323000000000000000">
Privacy</A>
</H2>
[<A
 HREF="mastersthesis.html#4698190">TVH08</A>]
[<A
 HREF="mastersthesis.html#4471983">TW08</A>]

<P>
With the advent of Location Based Services (LBS) it is becoming increasingly difficult to control the extent in which a users locational information is used. Mix networks use short-term psudo-anonymous names to mask the identities of participants. With this mechanism, it becomes much more difficult for an adversary to correlate which actions in the system were performed by which users of the system.
[<A
 HREF="mastersthesis.html#6270872">LL12</A>]
[<A
 HREF="mastersthesis.html#freudiger2007mix">FRF<SUP>+</SUP>07</A>]
[<A
 HREF="mastersthesis.html#1032602">PHE02</A>]

<H2><A NAME="SECTION00324000000000000000">
Cloud Based Assisted Technologies</A>
</H2>

<P>
With the advance of internet connectivity, it is rare that modern mobile cellular devices are offline. If we are constantly in communication by means of a global (universal) IP address, It follows that we can achieve two things which we previously could not. 

<P>
First, we can synchronize our local data with the data of others in real-time. This lends itself to instant news aggregation, social media, e-mail, instant messaging, and even VoIP technologies. This is not in itself extremely surprising as these features have existed in the scope of the desktop application since the dawn of always on high-speed broadband connections, but the ability to bring this to a widely distributed array of mobile devices brings the connectivity of our society (and the speed of information travel as a result) to an all new level.

<P>
Second, we can now outsource services which are not desired or capable of running on the mobile device to another computer. While this is typically (as of yet) a cloud service provider's machine, it is reasonable to consider that over time software will develop which allows users to host their own content from a simple always-on home computer which serves as a personal hub for content including but not limited to public social media, geosecure proxy access, private home surveillance, and data storage. By using strict private/public tags, and 'group' authentication on a server's data as well as RSS-style content aggregators, it should be possible to design a decentralized 'home cloud' service which can serve many useful purposes to a mobile user in the field.

<P>

<H1><A NAME="SECTION00330000000000000000">
Digital identification and modeling</A>
</H1>

<P>
As our current trend of minniaturized mobile networked devices continues, the ability to stream high definition real-time media from many simple cellular devices is beginning to unfold. With higher quality cameras emerging in consumer devices as well as faster network connnectivity emerging in the forms of 3G and 4G telecommunication services it is completely feasible (even today) to use two phones to achieve a long distance video conversation.

<P>
It is interesting to consider that if we can stream enough information between two points to effectively allow a user to 'see' and 'hear' what is in another location, then theoretically speaking we must have processed and moved (via the VoIP phone system) enough information about the two locations to effectively understand (to the same degree the two communicating people were capable) what is happening at each of the locations. We as humans don't think about the image processing we do on a daily basis. We take for granted the fact that, somewhere between the rods and cones of our eyes and the high-level understanding we have of our surroundings, a lot of information was processed, stored, and acted upon.

<P>
While the topic of Artificial Intelligence and Computer Vision is far from bridging this gap, impressive results have been found by re-thinking the ways we can facilitate enhanced modelling and identification techniques.

<P>

<H2><A NAME="SECTION00331000000000000000">
Bar codes &amp; QR codes</A>
</H2>
QR codes (or two-dimensional bar codes) can reference nearly anything. Ranging in size from 25x25 to 177x177, they are most often used to redirect a user to a URL containing anything from videos to product info to social media content [<A
 HREF="mastersthesis.html#6182398">ALYY11</A>]
Basic compression is done using run-length coding, where sequences of identical values are replaced with a single instance of that value followed by the repetition count. [<A
 HREF="mastersthesis.html#6182398">ALYY11</A>]

<H2><A NAME="SECTION00332000000000000000">
RFID; NFC</A>
</H2>
[<A
 HREF="mastersthesis.html#5340296">PJ09</A>]

<H2><A NAME="SECTION00333000000000000000">
real-time digital modeling</A>
</H2>

<P>

<H3><A NAME="SECTION00333100000000000000">
Triangulation &amp; Accelerometer based Orientated Positioning</A>
</H3>

<P>
If many radio-enabled devices (whether via wifi, bluetooth, gps, or other means) are within communication range, it is possible to use time-synchronized signals to triangulate the positions of the devices with respect to each other. If any of these devices were equipped with a GPS, it could communicate this information and allow neighbors to take their local position data (respective to each other) and place them globally.

<P>
If any of these devices were equipped with an accelerometer and a camera, it is theoretically possible to calculate a position and orientation vector for the camera, effectively letting the device give extremely precise descriptions of what region of space is being recorded. This information (spatial meta-data, as well as the raw audio/video data) can be combined with other similar information from the region in order to provide high-fidelity reconstructions of recorded events. This information can be further augmented by other sensory input such as spectroscopes, depth-sensors, and environment monitors.

<P>
Time-synchronization techniques are limited by the sample speed of the measuring apparatus' clock. Determining distance by means of the recieved signal strenth do not require a clock, however the distance cannot be deduced from the signal strength alone without a calibration phase which takes into account the variance of each device's radio signal.

<P>

<H3><A NAME="SECTION00333200000000000000">
Stitching multiple images/videos together</A>
</H3>
In cases where images are not aligned, unwanted artifacts can be produced. Color and lighting inconsistencies can also be introduced which would create an unbalanced effect. A lack of references and identifiable control points can also make it difficult to correctly position image fragments. [<A
 HREF="mastersthesis.html#4359344">JT08</A>]

<P>
[<A
 HREF="mastersthesis.html#5397590">XP09</A>]

<H3><A NAME="SECTION00333300000000000000">
Depth matricies/maps for 3D imaging</A>
</H3>
Depth imaging techniques have only previously existed in costly special-purpose applications. With the spread of large-scale production on Time-Of-Flight sensors (specifically, the Microsoft Kinect) the accessability and spread of these devices has grown considerably. Time-Of-Flight technology involves measuring the delay between sending and recieving an infared signal. With this information, triangulation techniques can be used to measure the depth between the device and the target. 

<P>
These devices have been designed with object recognition in mind and are not particularly suited for 3d scanning applications. Low resolution and a large amount of noise are certainly factors when re-purposing these technologies for scanning[<A
 HREF="mastersthesis.html#6296662">CST<SUP>+</SUP>13</A>] 

<H2><A NAME="SECTION00334000000000000000">
Image recognition and classification</A>
</H2>
Image recognition is a complex problem often approached with a neural network model or similar fuzzy categorical organizer. If RGB information is augmented with depth information we can achieve much better results than if we were to rely on RGB information alone.

<H3><A NAME="SECTION00334100000000000000">
Vector Quantization</A>
</H3>

<H3><A NAME="SECTION00334200000000000000">
Uncertainty / Fuzzy Logic</A>
</H3>

<H3><A NAME="SECTION00334300000000000000">
Improved accuracy through domain-specific environments/contexts</A>
</H3>
telepresence surgery model - live data can be collected, then used to simulate the procedure virtually. 
This effectively allows us to generate training programs which are extremely accurate within the domain of the live collected data.

<P>
Imagine a doctor in front of device operates instrumentation which performs surgery on a remote patient.
instrumentation includes a wide variety of I/O (controls and sensory output via microphone, video, and even tactic feedback)
Assume access to highly detailed descriptions of our I/O over the duration of many operations (live experience captures)
The challenge is to make a virtual model of the operating procedure in which the doctor can interact with a virtual patient in a way which is synonymous with the standard interactions they would encounter with a live patient.

<P>
The challenge is considered 'solved' when the doctor cannot differentiate between a live patient telepresence experience and a simulated patient telepresence experience
I call this challenge the "Telepresence Turing Test", and it can be applied to any activity or domain in which telepresence can augment.

<P>
This challenge has a few interesting unknowns.

<P>
How 'synonymous' with live data can we realistically make the experience?

<P>
What are the most important factors we need to capture in our training data? What instrumentation can best capture those factors?

<P>
From the training data (live experience captures), how can we best create and improve upon a simulation model?[<A
 HREF="mastersthesis.html#391769">GHJS95</A>] 

<P>

<H1><A NAME="SECTION00340000000000000000">
Identifying the Major Elements for a Lightweight Mobile Telepresence System</A>
</H1>

<P>
Lightweight mobile telepresence systems is a rapidly-evolving concept. Traditionally we have been bound by heavy and cumbersome desktop hardware, high-latency, and low network throughput. As these barriers have been reduced and removed, we have begun to redefine what it means to be connected.

<P>
Always-Available network communication

<P>
Real-Time video streaming

<P>
Geolocational services

<P>
Screen sharing (also useful for remote presentation)

<P>
File sharing (p2p for reducing infrastructure and bottle necks)

<P>
User/Group management and authentication

<P>

<H1><A NAME="SECTION00350000000000000000">
Summary</A>
</H1>

<P>
In conclusion, we are capable of much more than what is currently offered in terms of increasing the fidelity of our telepresence systems. On top of the increase in raw information storage capabilities, improvements in sensors as well as interactive peripherals have reshaped the way we use technology. If current trends continue we will soon find ourselves in a high density and highly distributed network of minniature devices, both as stand alone technologies (such as currently emerging smart phones) as well as embedded into every day consumer objects (as is the case with RFID tagging, QR coded items, and micro-controller enabled electronics). With this emerging paradigm it becomes much easier for computers to identify and process the objects around them, leading the way to many new modelling and digitizing techniques.

<P>

<H1><A NAME="SECTION00400000000000000000">
Design Document</A>
</H1>

<P>

<H1><A NAME="SECTION00410000000000000000">
Goals</A>
</H1>

<P>
Mobile Telepresence device which can traverse flat open space.
Battery operated
An on board camera

<P>
A phone application which can view the camera and control the base device

<P>
A system used to aid in telepresence integration for the remote viewee

<P>

<H1><A NAME="SECTION00420000000000000000">
Design</A>
</H1>

<UL>
<LI>Telepresence device equipped with ARM board controller which drives servos
</LI>
<LI>Telepresence device also equipped with webcam and microphone
</LI>
<LI>Android (Platform agnostic?) device on the client side for viewing live video and audio from telepresence device
</LI>
<LI>Client side android device capable of driving the telepresence device
</LI>
<LI>Computer vision techniques gives the viewer extended interactions with the remote environment. 

<P>
</LI>
<LI>For example, a QR code the telepresence device encounters can open the resource on the viewers mobile device
</LI>
<LI>For example, facial tracking techniques can be used to alert the remote viewer when somebody has entered the frame

<P>
</LI>
<LI>augmented data systems applied to telecommunications and mobile telepresence

<P>
</LI>
<LI>tons of data is available everywhere. There are many types of data and ways to visualize them and helpful calculations that can be done.

<P>
</LI>
<LI>'live' real time data is becoming more dense. from traditional IRC to VoIP and video streaming, bandwidth has come a long way

<P>
</LI>
<LI>This live data can be parsed and processed immediately upon arrival

<P>
</LI>
<LI>if we can process useful derivitive information in real-time with a small enough (how small?) latency, we can act on the results in a real-time manner.

<P>
</LI>
</UL>
<H1><A NAME="SECTION00430000000000000000">
Networking</A>
</H1>

<P>
<IMG
  WIDTH="473" HEIGHT="325" ALIGN="BOTTOM" BORDER="0"
 SRC="./networkProfile_Labeled.png"
 ALT="Image networkProfile_Labeled">

<P>

<H2><A NAME="SECTION00431000000000000000">
I/O Limitations</A>
</H2>
		
<UL>
<LI>hardware needs high bandwidth, low latency
</LI>
<LI>how many video streams simultaneously?
</LI>
<LI>latency? Client&lt;-&gt;DSP&lt;-&gt;Hardware vs Client&lt;-&gt;Hardware direct
</LI>
<LI>performance trade-offs of doing DSP on robot or mobile device?
		
</LI>
</UL>
<H3><A NAME="SECTION00431100000000000000">
optimizing performance</A>
</H3>
			
<UL>
<LI>video compression
</LI>
<LI>audio compression
</LI>
<LI>measure opencv processing latency
</LI>
<LI>opencv gpgpu techniques
			
</LI>
</UL>
<H2><A NAME="SECTION00432000000000000000">
Formal Grammar</A>
</H2>
		develop a formal grammar to convey information from each module to the next
		
<H3><A NAME="SECTION00432100000000000000">
Commands</A>
</H3>
			left,right,forward,backward
			toggle mute
			connect
		
<H3><A NAME="SECTION00432200000000000000">
Data</A>
</H3>
			audio
			video
			augmented overlay (another video stream)
		
<H3><A NAME="SECTION00432300000000000000">
Multiple Roles</A>
</H3>
			in some cases, data and commands are not divisible. eg: signal processing allows for commands to be encoded in video

<P>

<H1><A NAME="SECTION00440000000000000000">
Hardware</A>
</H1>
	The hardware component of the project is largely an exercise in manufacturing both structural and electronic devices. Many of the parts are off-the-shelf, while others will be hand crafted. The hardware for the project will consist of three parts. 

<P>
The first part is a simple smart phone. While a web-enabled client could technically be used on a desktop (and in practice it could be) a mobile phone allows the user a great deal of flexibility and mobility. Assuming the user has access to a broadband connection, the client application should be simple enough to run on a small device. 
<IMG
  WIDTH="473" HEIGHT="204" ALIGN="BOTTOM" BORDER="0"
 SRC="./MobileGUI_Labeled.png"
 ALT="Image MobileGUI_Labeled">

<UL>
<LI>smart phone client (android?IOS?) optionally desktop (if web app)
</LI>
<LI>Web stream in &lt;video&gt;element served by computer vision module
</LI>
<LI>touchscreen controls for left, right, forward, and reverse
</LI>
<LI>mute button turns speakers on and off
</LI>
</UL>

<P>
The second part is the cloud server which does the signal processing. The computational requirements are not extreme, so a simple desktop computer will do for a laboratory environment.

<UL>
<LI>cloud computation unit
</LI>
<LI>Basically a desktop computer with a GPU more capable than any of the embedded mobile devices
</LI>
</UL>

<P>
The final part is the telepresence avatar.

<UL>
<LI>vehicle
</LI>
<LI>weld steel chassis
</LI>
<LI>2 servo drive
</LI>
<LI>raspberry pi piface interface acts as the controller
</LI>
<LI>pan / tilt? (maybe just tilt, tank already moves)
</LI>
<LI>camera
</LI>
<LI>standard webcam. 30fps streamed over wifi
</LI>
<LI>microphone
</LI>
<LI>built into camera
</LI>
<LI>speakers
</LI>
<LI>for speaking with remote users
</LI>
<LI>wifi
</LI>
<LI>usb wifi dongle.
</LI>
</UL>

<P>

<H1><A NAME="SECTION00450000000000000000">
Digital Signal Processing</A>
</H1>
<IMG
  WIDTH="473" HEIGHT="336" ALIGN="BOTTOM" BORDER="0"
 SRC="./signalProcessing.png"
 ALT="Image signalProcessing">

<UL>
<LI>computer vision module
</LI>
<LI>raw video stream gathered from telepresence hardware
</LI>
<LI>video analyzed using computer vision and pattern recognition (opencv)
</LI>
<LI>client connected to this module will recieve events via the audio/video analyzer
</LI>
<LI>scripting language? HighGUI enough?
</LI>
<LI>client can also potentially recieve augmented audio/video data with the DSP module overlay
</LI>
</UL>

<P>

<H2><A NAME="SECTION00451000000000000000">
Components</A>
</H2>

<P>
Sensors/Input - generate data from the environment. Keyboard/Mouse are basic sensors. Video Cameras are more complex yet they are becoming extremely common. Many other sensors exist. GPS, microphone, accelerometer, etc.

<P>
Processor - consists of a three part pipeline:
	Data Extraction - reads the raw sensor data somewhere between the sensors and the client. Pre-processing can be done on the raw data, reducing it's size
	Pattern Recognition - the pre-processed data can be analyzed and compared against a database of models. If the data matches the models (within a degree of tolerance) than properties can be assigned to the data
	Action/Augmentation - As properties are discovered in the live data stream, they will trigger actions within the system. Actions can then be sent along with the raw data towards the client device which is capable of acting on the augmented stream

<P>
Client/Output - will recieve the real-time data along with augmented action events. Action events will be acted on.

<P>
Actions can get as complex as you like. For example, the client's device could initiate a video call if it detects a series of actions such as "a presenter is in the room and the presenter is speaking and the client has just raised their hand"

<P>

<H2><A NAME="SECTION00500000000000000000">
Bibliography</A>
</H2><DL COMPACT><DD><P></P><DT><A NAME="6094998">AH11</A>
<DD>
A.P. Arias and U.D. Hanebeck.
<BR>Motion control of a semi-mobile haptic interface for extended range
  telepresence.
<BR>In <EM>Intelligent Robots and Systems (IROS), 2011 IEEE/RSJ
  International Conference on</EM>, pages 3053-3059, 2011.

<P></P><DT><A NAME="5054795">AKK09</A>
<DD>
J.J. Ahmad, H.A. Khan, and S.A. Khayam.
<BR>Energy efficient video compression for wireless sensor networks.
<BR>In <EM>Information Sciences and Systems, 2009. CISS 2009. 43rd
  Annual Conference on</EM>, pages 629-634, 2009.

<P></P><DT><A NAME="6182398">ALYY11</A>
<DD>
Hou A-Lin, Feng Yuan, and Geng Ying.
<BR>QR code image detection using run-length coding.
<BR>In <EM>Computer Science and Network Technology (ICCSNT), 2011
  International Conference on</EM>, volume&nbsp;4, pages 2130-2134, 2011.

<P></P><DT><A NAME="726589">AWZ98</A>
<DD>
A.&nbsp;Agah, R.&nbsp;Walker, and R.&nbsp;Ziemer.
<BR>A mobile camera robotic system controlled via a head mounted display
  for telepresence.
<BR>In <EM>Systems, Man, and Cybernetics, 1998. 1998 IEEE International
  Conference on</EM>, volume&nbsp;4, pages 3526-3531 vol.4, 1998.

<P></P><DT><A NAME="6007847">Bin11</A>
<DD>
Huang Bin.
<BR>The study of Mobile Education development based on 3G technique and
  Cloud Computing.
<BR>In <EM>Uncertainty Reasoning and Knowledge Engineering (URKE), 2011
  International Conference on</EM>, volume&nbsp;1, pages 86-89, 2011.

<P></P><DT><A NAME="6296662">CST<SUP>+</SUP>13</A>
<DD>
Yan Cui, S.&nbsp;Schuon, S.&nbsp;Thrun, D.&nbsp;Stricker, and C.&nbsp;Theobalt.
<BR>Algorithms for 3d shape scanning with a depth camera.
<BR><EM>Pattern Analysis and Machine Intelligence, IEEE Transactions
  on</EM>, 35(5):1039-1050, 2013.

<P></P><DT><A NAME="540147">CWKG96</A>
<DD>
D.&nbsp;Caldwell, A.&nbsp;Wardle, O.&nbsp;Kocak, and M.&nbsp;Goodwin.
<BR>Telepresence feedback and input systems for a twin armed mobile
  robot.
<BR><EM>Robotics Automation Magazine, IEEE</EM>, 3(3):29-38, 1996.

<P></P><DT><A NAME="6104414">EAM12</A>
<DD>
C.&nbsp;Escolano, J.M. Antelis, and J.&nbsp;Minguez.
<BR>A Telepresence Mobile Robot Controlled With a Noninvasive Brain
  #x2013;Computer Interface.
<BR><EM>Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE
  Transactions on</EM>, 42(3):793-804, 2012.

<P></P><DT><A NAME="freudiger2007mix">FRF<SUP>+</SUP>07</A>
<DD>
Julien Freudiger, Maxim Raya, M&#225;rk F&#233;legyh&#225;zi, Panos
  Papadimitratos, et&nbsp;al.
<BR>Mix-zones for location privacy in vehicular networks.
<BR>In <EM>Proceedings of the first international workshop on wireless
  networking for intelligent transportation systems (Win-ITS)</EM>, 2007.

<P></P><DT><A NAME="391769">GHJS95</A>
<DD>
P.S. Green, J.W. Hill, J.F. Jensen, and A.&nbsp;Shah.
<BR>Telepresence surgery.
<BR><EM>Engineering in Medicine and Biology Magazine, IEEE</EM>,
  14(3):324-329, 1995.

<P></P><DT><A NAME="5970856">HCRM11</A>
<DD>
M.&nbsp;Hincapie, A.&nbsp;Caponio, H.&nbsp;Rios, and E.G. Mendivil.
<BR>An introduction to augmented reality with applications in
  aeronautical maintenance.
<BR>In <EM>Transparent Optical Networks (ICTON), 2011 13th International
  Conference on</EM>, pages 1-4, 2011.

<P></P><DT><A NAME="4469080">Hos07</A>
<DD>
W.&nbsp;Hosny.
<BR>Power engineering mobile education technology.
<BR>In <EM>Universities Power Engineering Conference, 2007. UPEC 2007.
  42nd International</EM>, pages 971-974, 2007.

<P></P><DT><A NAME="4359344">JT08</A>
<DD>
Jiaya Jia and Chi-Keung Tang.
<BR>Image Stitching Using Structure Deformation.
<BR><EM>Pattern Analysis and Machine Intelligence, IEEE Transactions
  on</EM>, 30(4):617-631, 2008.

<P></P><DT><A NAME="5710522">Kim11</A>
<DD>
S.&nbsp;Kim.
<BR>Cellular network bandwidth management scheme by using nash
  bargaining solution.
<BR><EM>Communications, IET</EM>, 5(3):371-380, 2011.

<P></P><DT><A NAME="1300874">KV04</A>
<DD>
Sungwook Kim and P.K. Varshney.
<BR>An integrated adaptive bandwidth-management framework for
  QoS-sensitive multimedia cellular networks.
<BR><EM>Vehicular Technology, IEEE Transactions on</EM>, 53(3):835-846,
  2004.

<P></P><DT><A NAME="6270872">LL12</A>
<DD>
Xinxin Liu and Xiaolin Li.
<BR>Privacy Preserving Techniques for Location Based Services in Mobile
  Networks.
<BR>In <EM>Parallel and Distributed Processing Symposium Workshops PhD
  Forum (IPDPSW), 2012 IEEE 26th International</EM>, pages 2474-2477, 2012.

<P></P><DT><A NAME="4801602">LLD09</A>
<DD>
Limin Liu, Zhen Li, and E.J. Delp.
<BR>Efficient and Low-Complexity Surveillance Video Compression Using
  Backward-Channel Aware Wyner-Ziv Video Coding.
<BR><EM>Circuits and Systems for Video Technology, IEEE Transactions
  on</EM>, 19(4):453-465, 2009.

<P></P><DT><A NAME="1376696">LYC04</A>
<DD>
Kam-Yiu Lam, Joe Yuen, and E.&nbsp;Chan.
<BR>On using buffered bandwidth to support real-time mobile video
  playback in cellular networks.
<BR>In <EM>Multimedia Software Engineering, 2004. Proceedings. IEEE
  Sixth International Symposium on</EM>, pages 466-473, 2004.

<P></P><DT><A NAME="1032602">PHE02</A>
<DD>
Sang&nbsp;Yun Park, Moon&nbsp;Seog Han, and Young&nbsp;Ik Eom.
<BR>An efficient authentication protocol supporting privacy in mobile
  computing environments.
<BR>In <EM>High Speed Networks and Multimedia Communications 5th IEEE
  International Conference on</EM>, pages 332-334, 2002.

<P></P><DT><A NAME="5340296">PJ09</A>
<DD>
R.&nbsp;Pathak and S.&nbsp;Joshi.
<BR>Recent trends in RFID and a java based software framework for its
  integration in mobile phones.
<BR>In <EM>Internet, 2009. AH-ICI 2009. First Asian Himalayas
  International Conference on</EM>, pages 1-5, 2009.

<P></P><DT><A NAME="4297087">SlZSSyX07</A>
<DD>
Zhao Shu-long, You Zhi-Sheng, Lan Shi-yong, and Zhou Xin.
<BR>An Improved Video Compression Algorithm for Lane Surveillance.
<BR>In <EM>Image and Graphics, 2007. ICIG 2007. Fourth International
  Conference on</EM>, pages 224-229, 2007.

<P></P><DT><A NAME="4698190">TVH08</A>
<DD>
Lei Tang, S.&nbsp;Vrbsky, and Xiaoyan Hong.
<BR>Collaborated Camouflaging Mobility for Mobile Privacy.
<BR>In <EM>Global Telecommunications Conference, 2008. IEEE GLOBECOM
  2008. IEEE</EM>, pages 1-5, 2008.

<P></P><DT><A NAME="4471983">TW08</A>
<DD>
Caimu Tang and D.O. Wu.
<BR>Mobile Privacy in Wireless Networks-Revisited.
<BR><EM>Wireless Communications, IEEE Transactions on</EM>, 7(3):1035-1042,
  2008.

<P></P><DT><A NAME="5397590">XP09</A>
<DD>
Yingen Xiong and K.&nbsp;Pulli.
<BR>Sequential image stitching for mobile panoramas.
<BR>In <EM>Information, Communications and Signal Processing, 2009.
  ICICS 2009. 7th International Conference on</EM>, pages 1-5, 2009.
</DL>

<BR><HR>

</BODY>
</HTML>
