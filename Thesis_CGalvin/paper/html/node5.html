<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2008 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Microcontrollers &amp; customized System on a Chip (SoaC) components</TITLE>
<META NAME="description" CONTENT="Microcontrollers &amp; customized System on a Chip (SoaC) components">
<META NAME="keywords" CONTENT="mastersthesis">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2008">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="mastersthesis.css">

<LINK REL="next" HREF="node6.html">
<LINK REL="previous" HREF="node4.html">
<LINK REL="up" HREF="node4.html">
<LINK REL="next" HREF="node6.html">
</HEAD>

<BODY >
<!--Navigation Panel-->
<A NAME="tex2html128"
  HREF="node6.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/lib/latex2html/icons/next.png"></A> 
<A NAME="tex2html124"
  HREF="node4.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/lib/latex2html/icons/up.png"></A> 
<A NAME="tex2html118"
  HREF="node4.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/lib/latex2html/icons/prev.png"></A> 
<A NAME="tex2html126"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="file:/usr/lib/latex2html/icons/contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html129"
  HREF="node6.html">Mobile phones &amp; Cellular</A>
<B> Up:</B> <A NAME="tex2html125"
  HREF="node4.html">Emerging Mobile Technologies</A>
<B> Previous:</B> <A NAME="tex2html119"
  HREF="node4.html">Emerging Mobile Technologies</A>
 &nbsp; <B>  <A NAME="tex2html127"
  HREF="node1.html">Contents</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->

<H2><A NAME="SECTION00311000000000000000">
Microcontrollers &amp; customized System on a Chip (SoaC) components</A>
</H2>

<P>
Microcontrollers have become the de-facto platform for lightweight, mobile, and miniaturized devices. Much of the power of microcontrollers has been achieved by clever specializations and optimizations of the CPU unit. By diverging from the Desktop model where raw power takes precedence over power consuption, there are now a variety of architectures and specialized components that are well-suited to mobility. In fact there is now a wide spectrum of hardware ranging from high performance to low power consumption. With server and desktop harware on one end of the spectrum, we have recently expanded the power efficient end with a range of miniaturized ARM general-purpose CPUs capable of of supporting a general-purpose operating system and related peripheral components in an extremely small enclosure. To continue down the spectrum we depart from a traditional operating system and move towards programmable integrated circuits (PIC) and pure-hardware components which perform more specialized tasks using even less space and power.

<P>
Because of this balance between portability and power, it is important to keep in mind the practical limitations on the applications a device can support. While a computationally demanding task such as image processing, we are unlikely to get satisfactory results with a PIC controller, yet the current generation of ARM controllers which are recently emerging are just beginning to practically handle these tasks.

<P>
An interesting idea to consider is delegation of the heavy processing to a more capable device. However, this strategy simply moves the problem from the computational I/O boundaries of the device towards the networking I/O boundaries. To achieve practical results, we can use a hybrid approach by allowing the device to pre-process the data (assuming such a pre-processing can reduce the size of the raw data, preferrably an order of magnitude or more) before sending the derived result over the network to a more capable device.

<P>
Moving from our internal capabilities towards our external interactivity, we are capable of hooking up a wide array of sensors, displays, and feedback devices. From motors to spectroscopy sensors to audio capture and processing (and many, many more) the possibilities are limited only by the fidelity/accuracy of the sensors around us and our own creativity.

<P>
Before we wander into fanciful and eclectic descriptions of advanced robots and artificial intelligence which could easily be the plot of the next sci-fi thriller, we should take note of the challenges and limitations of today's robotic and so-called 'smart' devices. For example, to apply the concept of tele-robotics (where the telepresence user is represented and mimiced as a robotic avatar on the opposite end) the following challenges have been identified and discussed:

<P>
<BLOCKQUOTE>
In this telepresence domain, as remote manipulators become more sophisticated and the tasks they undertake become more complex. Three main sensory obstacles to effective widespread use have been identified [15]:
	</BLOCKQUOTE>
<OL>
<LI>The detection of sensory signals
</LI>
<LI>The feedback of real-time sensory information to the operator
</LI>
<LI>The presentation of this information in a form that can be easily detected, processed by the brain as a reflex action and responded to, since an excessive need for thought would detract from performance of the primary task.
	
</LI>
</OL><BLOCKQUOTE>
	
<DIV ALIGN="RIGHT">
[<A
 HREF="node25.html#540147">CWKG96</A>]
	
</DIV>
</BLOCKQUOTE>

<P>
Aside from end-to-end synchronization and operator awareness, the question of usefulness and usability must also be addressed. Applicability has been fomrmally described as follows:

<P>
<P>
<BLOCKQUOTE>The main objective of tele-robotics has been to develop methodologies for the control of robots at remote sites by human users at local sites. Tele-robots are suitable in certain situations such as:
</BLOCKQUOTE>
<OL>
<LI>The robot must operate in environments that are hazardous to human health.
</LI>
<LI>The robot must operate at a scale that is much smaller or larger than the human size and scale.
</LI>
<LI>The robot must operate in a location where it would be too costly for the human to be present (in terms of budget, timing requirements, and human safety).
</LI>
</OL><BLOCKQUOTE>
	
<DIV ALIGN="RIGHT">
[<A
 HREF="node25.html#726589">AWZ98</A>]
	
</DIV>
</BLOCKQUOTE>

<P>
It begins to become clear that tele-robotics is not an idea that would lend itself to any situation. The reduction in fidelity we introduce by using a proxy or avatar robot is severe enough that we have only found successful use-cases in well-defined environments such as laboratories and factories where there are little or no unknown variables. Situations where there are people in close proximity to the robots are often too dangerous for the technology to be applied, and furthermore the decrease in the awareness a user would have with their remote surroundings makes the potential for interaction very limited. This barrier to adoption can be seen echoed in the following passage:

<P>
<BLOCKQUOTE>
Traditionally most robotic applications have involved the use of single static (non-mobile) manipulator platforms, with this technique being particularly suited to applications where the actual task is relatively well defined, the work volume is limited and safety considerations make even slightly “unexpected” motions totally unacceptable.
	
<DIV ALIGN="RIGHT">
[<A
 HREF="node25.html#540147">CWKG96</A>]
	
</DIV>
</BLOCKQUOTE>

<P>
<BLOCKQUOTE>

	...the BCI collects the EEG brain activity and decodes the user's intentions, which are transferred to the robot via the Internet. The robot autonomously executes the orders using the navigation system (implemented with a combination of dynamic online grid maping with scan matching, dynamic path planning, and obstacle avoidance) or the camera orientation system. Thus, the shared-control strategy is built by means  of the mental selection of robot navigation or active visual exploration task-related orders, which can be autonomously executed by the robot.
	
<DIV ALIGN="RIGHT">
[<A
 HREF="node25.html#6104414">EAM12</A>]
	
</DIV>
</BLOCKQUOTE>

<P>
The idea of presence fidelity can be considered a continuum. We can consider verbal descriptions and printed material on the low-fidelity end of the spectrum, while actual presence would be on the highest end. Our goal is to bring virtual telepresence farther along this continuum until it is as close to actual presence as possible [<A
 HREF="node25.html#726589">AWZ98</A>]

<P>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html128"
  HREF="node6.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="file:/usr/lib/latex2html/icons/next.png"></A> 
<A NAME="tex2html124"
  HREF="node4.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="file:/usr/lib/latex2html/icons/up.png"></A> 
<A NAME="tex2html118"
  HREF="node4.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="file:/usr/lib/latex2html/icons/prev.png"></A> 
<A NAME="tex2html126"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="file:/usr/lib/latex2html/icons/contents.png"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html129"
  HREF="node6.html">Mobile phones &amp; Cellular</A>
<B> Up:</B> <A NAME="tex2html125"
  HREF="node4.html">Emerging Mobile Technologies</A>
<B> Previous:</B> <A NAME="tex2html119"
  HREF="node4.html">Emerging Mobile Technologies</A>
 &nbsp; <B>  <A NAME="tex2html127"
  HREF="node1.html">Contents</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>

2013-06-15
</ADDRESS>
</BODY>
</HTML>
